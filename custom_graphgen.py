#!/usr/bin/env python3
"""
è‡ªå®šä¹‰ GraphGen ç±»ï¼Œæ”¯æŒä½¿ç”¨å¤–éƒ¨çŸ¥è¯†å›¾è°±
"""

import os
import time
import networkx as nx
from dataclasses import dataclass
from typing import Dict

from graphgen.graphgen import GraphGen
from graphgen.models.storage.networkx_storage import NetworkXStorage
from graphgen.utils import logger


@dataclass
class CustomGraphGen(GraphGen):
    """æ”¯æŒå¤–éƒ¨çŸ¥è¯†å›¾è°±çš„ GraphGenï¼Œæ— traineeæ¨¡å¼"""
    
    external_graph_path: str = None
    skip_kg_building: bool = True  # è·³è¿‡çŸ¥è¯†å›¾è°±æ„å»ºæ­¥éª¤
    no_trainee_mode: bool = True   # æ— traineeæ¨¡å¼ï¼Œä¸ä½¿ç”¨traineeå®¢æˆ·ç«¯
    
    def __post_init__(self):
        """åˆå§‹åŒ–ï¼ŒåŠ è½½å¤–éƒ¨çŸ¥è¯†å›¾è°±"""
        if self.no_trainee_mode:
            # æ— traineeæ¨¡å¼ï¼Œç›´æ¥ä½¿ç”¨æœ€å°åŒ–åˆå§‹åŒ–
            logger.info("ğŸš€ å¯ç”¨æ— traineeæ¨¡å¼ï¼Œä½¿ç”¨æœ€å°åŒ–åˆå§‹åŒ–")
            self._init_minimal()
        else:
            # æ ‡å‡†æ¨¡å¼ï¼Œå…ˆå°è¯•è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
            try:
                super().__post_init__()
            except Exception as e:
                # å¦‚æœ trainee ç›¸å…³ç¯å¢ƒå˜é‡ç¼ºå¤±ï¼Œåˆ›å»ºä¸€ä¸ªæœ€å°åŒ–çš„å®ä¾‹
                logger.warning(f"Trainee å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥ï¼Œå¯èƒ½æ˜¯ç¯å¢ƒå˜é‡ç¼ºå¤±: {e}")
                self._init_minimal()
        
        # å¦‚æœæä¾›äº†å¤–éƒ¨å›¾è°±è·¯å¾„ï¼Œåˆ™åŠ è½½å®ƒ
        if self.external_graph_path:
            self._load_external_graph()
    
    def _init_minimal(self):
        """æœ€å°åŒ–åˆå§‹åŒ–ï¼Œä¸ä¾èµ– trainee å®¢æˆ·ç«¯"""
        from graphgen.models import JsonKVStorage, JsonListStorage, NetworkXStorage, OpenAIClient, Tokenizer
        
        self.tokenizer_instance: Tokenizer = self.tokenizer_instance or Tokenizer(
            model_name=os.getenv("TOKENIZER_MODEL", "cl100k_base")
        )

        # åªæœ‰åœ¨æä¾›äº† API key æ—¶æ‰åˆ›å»º OpenAI å®¢æˆ·ç«¯
        synthesizer_api_key = os.getenv("SYNTHESIZER_API_KEY")
        if synthesizer_api_key:
            self.synthesizer_llm_client: OpenAIClient = (
                self.synthesizer_llm_client
                or OpenAIClient(
                    model_name=os.getenv("SYNTHESIZER_MODEL", "gpt-3.5-turbo"),
                    api_key=synthesizer_api_key,
                    base_url=os.getenv("SYNTHESIZER_BASE_URL"),
                    tokenizer=self.tokenizer_instance,
                )
            )
            logger.info("âœ… Synthesizer LLM å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        else:
            logger.warning("âš ï¸  æœªæä¾› SYNTHESIZER_API_KEYï¼Œsynthesizer_llm_client å°†è®¾ä¸º None")
            logger.info("ğŸ’¡ å¦‚éœ€å®Œæ•´åŠŸèƒ½ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡: export SYNTHESIZER_API_KEY='your_api_key'")
            self.synthesizer_llm_client = None

        # æ— traineeæ¨¡å¼ï¼štrainee_llm_client å§‹ç»ˆè®¾ä¸º None
        self.trainee_llm_client = None
        logger.info("ğŸš« æ— traineeæ¨¡å¼ï¼štrainee_llm_client è®¾ä¸º None")

        self.full_docs_storage: JsonKVStorage = JsonKVStorage(
            self.working_dir, namespace="full_docs"
        )
        self.text_chunks_storage: JsonKVStorage = JsonKVStorage(
            self.working_dir, namespace="text_chunks"
        )
        self.graph_storage: NetworkXStorage = NetworkXStorage(
            self.working_dir, namespace="graph"
        )
        self.search_storage: JsonKVStorage = JsonKVStorage(
            self.working_dir, namespace="search"
        )
        self.rephrase_storage: JsonKVStorage = JsonKVStorage(
            self.working_dir, namespace="rephrase"
        )
        self.qa_storage: JsonListStorage = JsonListStorage(
            os.path.join(self.working_dir, "data", "graphgen", f"{self.unique_id}"),
            namespace="qa",
        )
    
    def _load_external_graph(self):
        """åŠ è½½å¤–éƒ¨çŸ¥è¯†å›¾è°±"""
        if not os.path.exists(self.external_graph_path):
            raise FileNotFoundError(f"å¤–éƒ¨çŸ¥è¯†å›¾è°±æ–‡ä»¶ä¸å­˜åœ¨: {self.external_graph_path}")
        
        try:
            logger.info(f"æ­£åœ¨åŠ è½½å¤–éƒ¨çŸ¥è¯†å›¾è°±: {self.external_graph_path}")
            external_graph = nx.read_graphml(self.external_graph_path)
            
            # æ›¿æ¢é»˜è®¤çš„å›¾å­˜å‚¨
            self.graph_storage._graph = external_graph
            
            logger.info(
                f"âœ… å¤–éƒ¨çŸ¥è¯†å›¾è°±åŠ è½½æˆåŠŸ: {external_graph.number_of_nodes()} ä¸ªèŠ‚ç‚¹, "
                f"{external_graph.number_of_edges()} æ¡è¾¹"
            )
            
            # æ˜¾ç¤ºä¸€äº›ç»Ÿè®¡ä¿¡æ¯
            self._show_graph_stats(external_graph)
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½å¤–éƒ¨çŸ¥è¯†å›¾è°±å¤±è´¥: {e}")
            raise
    
    def _show_graph_stats(self, graph: nx.Graph):
        """æ˜¾ç¤ºå›¾è°±ç»Ÿè®¡ä¿¡æ¯"""
        # èŠ‚ç‚¹ç±»å‹ç»Ÿè®¡
        node_types = {}
        for node_id, node_data in graph.nodes(data=True):
            entity_type = node_data.get('entity_type', 'unknown')
            node_types[entity_type] = node_types.get(entity_type, 0) + 1
        
        logger.info("ğŸ“Š èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ:")
        for entity_type, count in sorted(node_types.items(), key=lambda x: x[1], reverse=True)[:10]:
            logger.info(f"   - {entity_type}: {count}")
        
        # è¿é€šæ€§ç»Ÿè®¡
        if graph.number_of_nodes() > 0:
            connected_components = list(nx.connected_components(graph))
            logger.info(f"ğŸ”— è¿é€šç»„ä»¶æ•°: {len(connected_components)}")
            if connected_components:
                largest_component_size = max(len(cc) for cc in connected_components)
                logger.info(f"ğŸ“ˆ æœ€å¤§è¿é€šç»„ä»¶å¤§å°: {largest_component_size}")
    
    async def insert(self, read_config: Dict = None, split_config: Dict = None):
        """
        é‡å†™æ’å…¥æ–¹æ³•ï¼Œæ”¯æŒè·³è¿‡çŸ¥è¯†å›¾è°±æ„å»ºå’Œæ— traineeæ¨¡å¼
        """
        if self.skip_kg_building:
            if self.external_graph_path:
                logger.info("â­ï¸  è·³è¿‡çŸ¥è¯†å›¾è°±æ„å»ºæ­¥éª¤ï¼Œä½¿ç”¨å¤–éƒ¨çŸ¥è¯†å›¾è°±")
                # ç›´æ¥è°ƒç”¨æ’å…¥å®Œæˆå›è°ƒ
                await self._insert_done()
                return True
            else:
                logger.info("â­ï¸  è·³è¿‡çŸ¥è¯†å›¾è°±æ„å»ºæ­¥éª¤ï¼ˆæ— å¤–éƒ¨å›¾è°±ï¼‰")
                return True
        
        # å¦‚æœéœ€è¦æ„å»ºçŸ¥è¯†å›¾è°±ï¼Œæ£€æŸ¥å‚æ•°å’Œå®¢æˆ·ç«¯
        if read_config is None or split_config is None:
            logger.warning("âš ï¸  read_config æˆ– split_config ä¸ºç©ºï¼Œè·³è¿‡çŸ¥è¯†å›¾è°±æ„å»º")
            return True
            
        if self.synthesizer_llm_client is None:
            logger.warning("âš ï¸  synthesizer_llm_client æœªåˆå§‹åŒ–ï¼Œè·³è¿‡çŸ¥è¯†å›¾è°±æ„å»º")
            logger.info("ğŸ’¡ å¦‚éœ€æ„å»ºçŸ¥è¯†å›¾è°±ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡: export SYNTHESIZER_API_KEY='your_openai_api_key'")
            return True
        
        # è°ƒç”¨åŸå§‹çš„æ’å…¥æ–¹æ³•
        return await super().insert(read_config, split_config)
    
    async def insert_additional_data(self, read_config: Dict, split_config: Dict):
        """
        åœ¨å¤–éƒ¨çŸ¥è¯†å›¾è°±åŸºç¡€ä¸Šæ’å…¥é¢å¤–æ•°æ®
        """
        if self.synthesizer_llm_client is None:
            logger.warning("âš ï¸  synthesizer_llm_client æœªåˆå§‹åŒ–ï¼Œè·³è¿‡é¢å¤–æ•°æ®æ’å…¥")
            logger.info("ğŸ’¡ å¦‚éœ€æ’å…¥é¢å¤–æ•°æ®ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡: export SYNTHESIZER_API_KEY='your_openai_api_key'")
            return
            
        logger.info("ğŸ“ åœ¨å¤–éƒ¨çŸ¥è¯†å›¾è°±åŸºç¡€ä¸Šæ’å…¥é¢å¤–æ•°æ®...")
        return await super().insert(read_config, split_config)
    
    def search(self, search_config: Dict):
        """
        é‡å†™æœç´¢æ–¹æ³•ï¼Œæ— traineeæ¨¡å¼ä¸‹ä»å¯æ­£å¸¸å·¥ä½œ
        æ³¨æ„ï¼šçˆ¶ç±»çš„searchæ–¹æ³•è¢«@async_to_sync_methodè£…é¥°ï¼Œæ‰€ä»¥è¿™é‡Œä¸éœ€è¦async/await
        """
        logger.info("ğŸ” æ‰§è¡Œæœç´¢æ“ä½œï¼ˆæ— traineeæ¨¡å¼ï¼‰...")
        return super().search(search_config)
    
    def quiz_and_judge(self, quiz_and_judge_config: Dict):
        """
        é‡å†™é—®ç­”æµ‹è¯•æ–¹æ³•ï¼Œæ— traineeæ¨¡å¼ä¸‹è·³è¿‡æ­¤æ­¥éª¤
        æ³¨æ„ï¼šçˆ¶ç±»çš„quiz_and_judgeæ–¹æ³•è¢«@async_to_sync_methodè£…é¥°ï¼Œæ‰€ä»¥è¿™é‡Œä¸éœ€è¦async/await
        """
        if self.no_trainee_mode or self.trainee_llm_client is None:
            if self.no_trainee_mode:
                logger.info("ğŸš« æ— traineeæ¨¡å¼ï¼šè·³è¿‡é—®ç­”æµ‹è¯•å’Œåˆ¤æ–­æ­¥éª¤")
            else:
                logger.warning("âš ï¸  Trainee å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè·³è¿‡é—®ç­”æµ‹è¯•å’Œåˆ¤æ–­æ­¥éª¤")
            return
        
        return super().quiz_and_judge(quiz_and_judge_config)
    
    def generate(self, partition_config: Dict, generate_config: Dict):
        """
        é‡å†™ç”Ÿæˆæ–¹æ³•ï¼Œå¤„ç†æ— traineeæ¨¡å¼å’Œç¼ºå°‘synthesizerå®¢æˆ·ç«¯çš„æƒ…å†µ
        æ³¨æ„ï¼šçˆ¶ç±»çš„generateæ–¹æ³•è¢«@async_to_sync_methodè£…é¥°ï¼Œæ‰€ä»¥è¿™é‡Œä¸éœ€è¦async/await
        """
        if self.synthesizer_llm_client is None:
            logger.error("âŒ æ— æ³•ç”Ÿæˆæ•°æ®ï¼šsynthesizer_llm_client æœªåˆå§‹åŒ–")
            logger.info("ğŸ’¡ è¯·è®¾ç½®ç¯å¢ƒå˜é‡: export SYNTHESIZER_API_KEY='your_openai_api_key'")
            logger.info("ğŸ’¡ æˆ–è€…ä½¿ç”¨ç®€åŒ–ç‰ˆç”Ÿæˆæ–¹æ¡ˆ")
            raise ValueError("synthesizer_llm_client æœªåˆå§‹åŒ–ï¼Œæ— æ³•è¿›è¡Œæ•°æ®ç”Ÿæˆ")
        
        # åœ¨æ— traineeæ¨¡å¼ä¸‹ï¼Œå¦‚æœä½¿ç”¨äº†åŸºäºlossçš„edge_samplingï¼Œéœ€è¦æ”¹ä¸ºrandom
        if (self.no_trainee_mode or self.trainee_llm_client is None):
            if (partition_config.get("method") == "ece" 
                and "method_params" in partition_config
                and partition_config["method_params"].get("edge_sampling") in ["max_loss", "min_loss"]):
                logger.warning("æ— traineeæ¨¡å¼ä¸‹ï¼Œedge_samplingä» '%s' æ”¹ä¸º 'random'ï¼Œå› ä¸ºæ²¡æœ‰losså±æ€§", 
                             partition_config["method_params"]["edge_sampling"])
                partition_config["method_params"]["edge_sampling"] = "random"
        
        logger.info("ğŸš€ å¼€å§‹ç”Ÿæˆæ•°æ®ï¼ˆæ— traineeæ¨¡å¼ï¼‰...")
        return super().generate(partition_config, generate_config)
    
    def generate_sync(self, partition_config: Dict, generate_config: Dict):
        """
        åŒæ­¥ç‰ˆæœ¬çš„ç”Ÿæˆæ–¹æ³•ï¼Œé¿å…äº‹ä»¶å¾ªç¯å†²çª
        """
        import asyncio
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨äº‹ä»¶å¾ªç¯ä¸­
        try:
            loop = asyncio.get_running_loop()
            # å¦‚æœå·²ç»åœ¨äº‹ä»¶å¾ªç¯ä¸­ï¼Œåˆ›å»ºä¸€ä¸ªä»»åŠ¡å¹¶ç­‰å¾…
            logger.warning("æ£€æµ‹åˆ°æ­£åœ¨è¿è¡Œçš„äº‹ä»¶å¾ªç¯ï¼Œåˆ›å»ºå¼‚æ­¥ä»»åŠ¡")
            
            # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
            task = loop.create_task(super().generate(partition_config, generate_config))
            
            # ä½¿ç”¨ nest_asyncio æ¥è¿è¡Œä»»åŠ¡
            try:
                import nest_asyncio
                nest_asyncio.apply()
                return loop.run_until_complete(task)
            except ImportError:
                # å¦‚æœæ²¡æœ‰ nest_asyncioï¼ŒæŠ›å‡ºé”™è¯¯è®©ä¸Šå±‚å¤„ç†
                raise RuntimeError("éœ€è¦å®‰è£… nest_asyncio æ¥å¤„ç†åµŒå¥—äº‹ä»¶å¾ªç¯")
            
        except RuntimeError as e:
            if "no running event loop" in str(e):
                # æ²¡æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯ï¼Œå¯ä»¥å®‰å…¨åœ°åˆ›å»ºæ–°çš„
                return asyncio.run(super().generate(partition_config, generate_config))
            else:
                raise
    
    def get_graph_summary(self) -> Dict:
        """è·å–å›¾è°±æ‘˜è¦ä¿¡æ¯"""
        graph = self.graph_storage._graph
        
        # åŸºæœ¬ç»Ÿè®¡
        summary = {
            'nodes': graph.number_of_nodes(),
            'edges': graph.number_of_edges(),
            'density': nx.density(graph) if graph.number_of_nodes() > 1 else 0,
        }
        
        # èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ
        node_types = {}
        for node_id, node_data in graph.nodes(data=True):
            entity_type = node_data.get('entity_type', 'unknown')
            node_types[entity_type] = node_types.get(entity_type, 0) + 1
        summary['node_types'] = node_types
        
        # è¿é€šæ€§ä¿¡æ¯
        if graph.number_of_nodes() > 0:
            connected_components = list(nx.connected_components(graph))
            summary['connected_components'] = len(connected_components)
            if connected_components:
                summary['largest_component_size'] = max(len(cc) for cc in connected_components)
        
        return summary
    
    def export_graph_statistics(self, output_file: str):
        """å¯¼å‡ºå›¾è°±ç»Ÿè®¡ä¿¡æ¯åˆ°æ–‡ä»¶"""
        import json
        
        summary = self.get_graph_summary()
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“Š å›¾è°±ç»Ÿè®¡ä¿¡æ¯å·²å¯¼å‡ºåˆ°: {output_file}")


def create_custom_config(
    external_graph_path: str,
    generation_mode: str = "atomic",
    data_format: str = "Alpaca",
    quiz_samples: int = 5,
    max_depth: int = 3,
    max_extra_edges: int = 5
) -> Dict:
    """
    åˆ›å»ºé€‚ç”¨äºå¤–éƒ¨çŸ¥è¯†å›¾è°±çš„é…ç½®
    
    Args:
        external_graph_path: å¤–éƒ¨çŸ¥è¯†å›¾è°±è·¯å¾„
        generation_mode: ç”Ÿæˆæ¨¡å¼ (atomic, aggregated, multi_hop, cot)
        data_format: æ•°æ®æ ¼å¼ (Alpaca, Sharegpt, ChatML)
        quiz_samples: æµ‹è¯•æ ·æœ¬æ•°é‡
        max_depth: æœ€å¤§éå†æ·±åº¦
        max_extra_edges: æœ€å¤§é¢å¤–è¾¹æ•°
    """
    config = {
        "read": {
            "input_file": []  # ç©ºæ–‡ä»¶åˆ—è¡¨ï¼Œå› ä¸ºä½¿ç”¨å¤–éƒ¨çŸ¥è¯†å›¾è°±
        },
        "split": {
            "chunk_size": 1024,
            "chunk_overlap": 100
        },
        "search": {
            "enabled": False,  # ç¦ç”¨æœç´¢ï¼Œç›´æ¥ä½¿ç”¨å¤–éƒ¨çŸ¥è¯†å›¾è°±
            "search_types": []
        },
        "quiz_and_judge": {
            "enabled": True,
            "quiz_samples": quiz_samples,
            "re_judge": False
        },
        "partition": {
            "method": "ece",
            "method_params": {
                "bidirectional": True,
                "edge_sampling": "max_loss",
                "expand_method": "max_width",
                "isolated_node_strategy": "ignore",
                "max_depth": max_depth,
                "max_extra_edges": max_extra_edges,
                "max_tokens": 256,
                "loss_strategy": "only_edge"
            }
        },
        "generate": {
            "mode": generation_mode,
            "data_format": data_format
        }
    }
    
    return config


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    import yaml
    import asyncio
    from dotenv import load_dotenv
    
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    
    # é…ç½®å‚æ•°
    EXTERNAL_GRAPH_PATH = "cache/external_graph.graphml"
    WORKING_DIR = "cache"
    
    async def main():
        # åˆ›å»ºé…ç½®
        config = create_custom_config(
            external_graph_path=EXTERNAL_GRAPH_PATH,
            generation_mode="atomic",
            data_format="Alpaca",
            quiz_samples=3,
            max_depth=2,
            max_extra_edges=3
        )
        
        # åˆ›å»ºè‡ªå®šä¹‰ GraphGen å®ä¾‹
        graph_gen = CustomGraphGen(
            external_graph_path=EXTERNAL_GRAPH_PATH,
            working_dir=WORKING_DIR,
            unique_id=int(time.time())
        )
        
        try:
            # æ’å…¥æ­¥éª¤ï¼ˆè·³è¿‡çŸ¥è¯†å›¾è°±æ„å»ºï¼‰
            await graph_gen.insert(
                read_config=config["read"], 
                split_config=config["split"]
            )
            
            # å¯é€‰ï¼šæœç´¢å¢å¼º
            if config["search"]["enabled"]:
                await graph_gen.search(search_config=config["search"])
            
            # é—®ç­”æµ‹è¯•å’Œåˆ¤æ–­
            if config["quiz_and_judge"]["enabled"]:
                await graph_gen.quiz_and_judge(quiz_and_judge_config=config["quiz_and_judge"])
            else:
                # å¦‚æœç¦ç”¨äº†é—®ç­”æµ‹è¯•ï¼Œéœ€è¦å°†edge_samplingè®¾ä¸ºrandomï¼Œå› ä¸ºæ²¡æœ‰losså±æ€§
                logger.warning("Quiz and Judge strategy is disabled. Edge sampling falls back to random.")
                if (config["partition"]["method"] == "ece" 
                    and "method_params" in config["partition"]):
                    config["partition"]["method_params"]["edge_sampling"] = "random"
            
            # ç”Ÿæˆæ•°æ®
            await graph_gen.generate(
                partition_config=config["partition"],
                generate_config=config["generate"]
            )
            
            # å¯¼å‡ºç»Ÿè®¡ä¿¡æ¯
            graph_gen.export_graph_statistics("cache/graph_statistics.json")
            
            print("âœ… æ•°æ®ç”Ÿæˆå®Œæˆï¼æ£€æŸ¥ cache/data/graphgen/ ç›®å½•")
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            raise
    
    # è¿è¡Œç¤ºä¾‹
    # asyncio.run(main())