# ä¸­é—´æ­¥éª¤ä¿å­˜åŠŸèƒ½ - å®Œæ•´è¯´æ˜

## ğŸ¯ åŠŸèƒ½æ¦‚è¿°

åœ¨ä½¿ç”¨ `run_youtu_json_kg.py` è¿è¡ŒGraphGenæ—¶ï¼Œç°åœ¨ä¼šè‡ªåŠ¨ä¿å­˜å„ä¸ªç”Ÿæˆæ¨¡å¼ä¸‹çš„ä¸­é—´æ­¥éª¤è¯¦ç»†ä¿¡æ¯ã€‚è¿™åŒ…æ‹¬ï¼š

- âœ… **Promptä¿¡æ¯**: æ¯ä¸ªæ­¥éª¤ä½¿ç”¨çš„å®Œæ•´prompt
- âœ… **LLMå“åº”**: æ¯ä¸ªæ­¥éª¤LLMè¿”å›çš„åŸå§‹å“åº”
- âœ… **å®ä½“å’Œå…³ç³»**: è¾“å…¥çš„çŸ¥è¯†å›¾è°±ä¿¡æ¯
- âœ… **æ­¥éª¤è¿½è¸ª**: å¤šæ­¥éª¤ç”Ÿæˆè¿‡ç¨‹çš„å®Œæ•´è¿½è¸ª

## ğŸ“ æ”¯æŒçš„æ¨¡å¼

### 1. Atomic æ¨¡å¼
ä¸€æ¬¡æ€§ä»å•ä¸ªå®ä½“æˆ–å…³ç³»ç”Ÿæˆé—®ç­”å¯¹

**ä¿å­˜çš„ä¸­é—´æ­¥éª¤:**
- è¾“å…¥çš„å®ä½“/å…³ç³»æè¿°
- é—®ç­”ç”Ÿæˆçš„prompt
- LLMçš„åŸå§‹å“åº”

### 2. Aggregated æ¨¡å¼
ä»å¤šä¸ªå®ä½“å’Œå…³ç³»çš„èšåˆä¿¡æ¯ç”Ÿæˆé—®ç­”å¯¹

**ä¿å­˜çš„ä¸­é—´æ­¥éª¤:**
- æ­¥éª¤1: é‡è¿°æ–‡æœ¬
  - å®ä½“å’Œå…³ç³»åˆ—è¡¨
  - é‡è¿°prompt
  - é‡è¿°åçš„ä¸Šä¸‹æ–‡
- æ­¥éª¤2: ç”Ÿæˆé—®é¢˜
  - é—®é¢˜ç”Ÿæˆprompt
  - ç”Ÿæˆçš„é—®é¢˜

### 3. Multi-hop æ¨¡å¼
ç”Ÿæˆéœ€è¦å¤šè·³æ¨ç†çš„é—®ç­”å¯¹

**ä¿å­˜çš„ä¸­é—´æ­¥éª¤:**
- å®ä½“å’Œå…³ç³»åˆ—è¡¨
- æ ¼å¼åŒ–çš„å®ä½“å’Œå…³ç³»å­—ç¬¦ä¸²
- å¤šè·³æ¨ç†prompt
- LLMçš„åŸå§‹å“åº”

### 4. CoT (Chain of Thought) æ¨¡å¼
ç”Ÿæˆå¸¦æœ‰æ¨ç†é“¾çš„é—®ç­”å¯¹

**ä¿å­˜çš„ä¸­é—´æ­¥éª¤:**
- æ­¥éª¤1: è®¾è®¡é—®é¢˜å’Œæ¨ç†è·¯å¾„
  - å®ä½“å’Œå…³ç³»åˆ—è¡¨
  - æ¨¡æ¿è®¾è®¡prompt
  - LLMè¿”å›çš„é—®é¢˜å’Œæ¨ç†è·¯å¾„
  - æå–çš„é—®é¢˜
  - æå–çš„æ¨ç†è·¯å¾„
- æ­¥éª¤2: ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
  - ç­”æ¡ˆç”Ÿæˆprompt
  - æœ€ç»ˆçš„CoTç­”æ¡ˆ

## ğŸš€ å¿«é€Ÿå¼€å§‹

### è¿è¡Œç¤ºä¾‹

```bash
# Atomicæ¨¡å¼
python run_youtu_json_kg.py \
  --json example_youtu_data.json \
  --mode atomic \
  --format Alpaca \
  --working-dir cache

# CoTæ¨¡å¼
python run_youtu_json_kg.py \
  --json example_youtu_data.json \
  --mode cot \
  --format Sharegpt \
  --working-dir cache

# Multi-hopæ¨¡å¼
python run_youtu_json_kg.py \
  --json example_youtu_data.json \
  --mode multi_hop \
  --format ChatML \
  --working-dir cache
```

### æŸ¥çœ‹ç»“æœ

ç”Ÿæˆçš„æ–‡ä»¶ä½äºï¼š
```
cache/data/graphgen/{unique_id}/qa.json
```

æ¯æ¡æ•°æ®éƒ½åŒ…å« `intermediate_steps` å­—æ®µã€‚

### æµ‹è¯•éªŒè¯

```bash
# è¿è¡Œæµ‹è¯•è„šæœ¬éªŒè¯ä¸­é—´æ­¥éª¤æ˜¯å¦æ­£ç¡®ä¿å­˜
python test_intermediate_steps.py cache/data/graphgen/{unique_id}/qa.json
```

æµ‹è¯•è„šæœ¬ä¼šï¼š
- æ£€æŸ¥æ‰€æœ‰æ•°æ®æ˜¯å¦åŒ…å«ä¸­é—´æ­¥éª¤
- ç»Ÿè®¡å„æ¨¡å¼çš„åˆ†å¸ƒ
- æ˜¾ç¤ºç¬¬ä¸€æ¡æ•°æ®çš„è¯¦ç»†ä¿¡æ¯
- åˆ†æpromptçš„ç»Ÿè®¡ä¿¡æ¯

## ğŸ“Š è¾“å‡ºæ ¼å¼ç¤ºä¾‹

### Alpacaæ ¼å¼ + Atomicæ¨¡å¼

```json
{
  "instruction": "ä»€ä¹ˆæ˜¯å…‰åˆä½œç”¨ï¼Ÿ",
  "input": "",
  "output": "å…‰åˆä½œç”¨æ˜¯æ¤ç‰©åˆ©ç”¨å…‰èƒ½å°†äºŒæ°§åŒ–ç¢³å’Œæ°´è½¬åŒ–ä¸ºè‘¡è„ç³–å’Œæ°§æ°”çš„è¿‡ç¨‹ã€‚",
  "intermediate_steps": {
    "mode": "atomic",
    "input_description": "å…‰åˆä½œç”¨: æ¤ç‰©åˆ©ç”¨å…‰èƒ½åˆæˆæœ‰æœºç‰©çš„è¿‡ç¨‹",
    "qa_generation_prompt": "You are given a text passage. Your task is to generate a question and answer (QA) pair...\n\nå…‰åˆä½œç”¨: æ¤ç‰©åˆ©ç”¨å…‰èƒ½åˆæˆæœ‰æœºç‰©çš„è¿‡ç¨‹",
    "raw_qa_response": "Question: ä»€ä¹ˆæ˜¯å…‰åˆä½œç”¨ï¼Ÿ\nAnswer: å…‰åˆä½œç”¨æ˜¯æ¤ç‰©åˆ©ç”¨å…‰èƒ½å°†äºŒæ°§åŒ–ç¢³å’Œæ°´è½¬åŒ–ä¸ºè‘¡è„ç³–å’Œæ°§æ°”çš„è¿‡ç¨‹ã€‚"
  }
}
```

### Sharegptæ ¼å¼ + CoTæ¨¡å¼

```json
{
  "conversations": [
    {"from": "human", "value": "è¯·è§£é‡Šå…‰åˆä½œç”¨çš„è¿‡ç¨‹"},
    {"from": "gpt", "value": "é¦–å…ˆï¼Œæ¤ç‰©é€šè¿‡å¶ç»¿ä½“å¸æ”¶å…‰èƒ½..."}
  ],
  "reasoning_path": "æ­¥éª¤1: è¯†åˆ«å…‰èƒ½çš„å¸æ”¶è¿‡ç¨‹\næ­¥éª¤2: åˆ†æåŒ–å­¦ååº”...",
  "intermediate_steps": {
    "mode": "cot",
    "community_id": 1,
    "entities": ["(å…‰åˆä½œç”¨: æ¤ç‰©çš„èƒ½é‡è½¬æ¢è¿‡ç¨‹)", "(å¶ç»¿ä½“: è¿›è¡Œå…‰åˆä½œç”¨çš„ç»†èƒå™¨)"],
    "relationships": ["(æ¤ç‰©) - [è¿›è¡Œ] -> (å…‰åˆä½œç”¨)", "(å…‰åˆä½œç”¨) - [å‘ç”Ÿåœ¨] -> (å¶ç»¿ä½“)"],
    "step1_template_design_prompt": "ä½ æ˜¯ä¸€ä½\"å…ƒæ¨ç†æ¶æ„å¸ˆ\"...",
    "step1_template_design_response": "é—®é¢˜ï¼šè¯·è§£é‡Šå…‰åˆä½œç”¨çš„è¿‡ç¨‹\næ¨ç†è·¯å¾„è®¾è®¡ï¼šæ­¥éª¤1: è¯†åˆ«å…‰èƒ½çš„å¸æ”¶è¿‡ç¨‹...",
    "step1_extracted_question": "è¯·è§£é‡Šå…‰åˆä½œç”¨çš„è¿‡ç¨‹",
    "step1_extracted_reasoning_path": "æ­¥éª¤1: è¯†åˆ«å…‰èƒ½çš„å¸æ”¶è¿‡ç¨‹\næ­¥éª¤2: åˆ†æåŒ–å­¦ååº”...",
    "step2_answer_generation_prompt": "æ ¹æ®ç»™å®šçš„çŸ¥è¯†å›¾è°±åŸå§‹ä¿¡æ¯åŠå·²ç”Ÿæˆçš„æ¨ç†è·¯å¾„...",
    "step2_final_answer": "é¦–å…ˆï¼Œæ¤ç‰©é€šè¿‡å¶ç»¿ä½“å¸æ”¶å…‰èƒ½..."
  }
}
```

### ChatMLæ ¼å¼ + Multi-hopæ¨¡å¼

```json
{
  "messages": [
    {"role": "user", "content": "æ¤ç‰©å¦‚ä½•é€šè¿‡å…‰åˆä½œç”¨äº§ç”Ÿæ°§æ°”ï¼Ÿ"},
    {"role": "assistant", "content": "æ¤ç‰©é€šè¿‡å…‰åˆä½œç”¨åˆ†è§£æ°´åˆ†å­ï¼Œé‡Šæ”¾æ°§æ°”..."}
  ],
  "intermediate_steps": {
    "mode": "multi_hop",
    "entities": ["æ¤ç‰©: è¿›è¡Œå…‰åˆä½œç”¨çš„ç”Ÿç‰©", "å…‰åˆä½œç”¨: èƒ½é‡è½¬æ¢è¿‡ç¨‹", "æ°§æ°”: å…‰åˆä½œç”¨çš„å‰¯äº§ç‰©"],
    "relationships": ["æ¤ç‰© -- å…‰åˆä½œç”¨: æ¤ç‰©è¿›è¡Œå…‰åˆä½œç”¨", "å…‰åˆä½œç”¨ -- æ°§æ°”: å…‰åˆä½œç”¨äº§ç”Ÿæ°§æ°”"],
    "entities_formatted": "1. æ¤ç‰©: è¿›è¡Œå…‰åˆä½œç”¨çš„ç”Ÿç‰©\n2. å…‰åˆä½œç”¨: èƒ½é‡è½¬æ¢è¿‡ç¨‹\n3. æ°§æ°”: å…‰åˆä½œç”¨çš„å‰¯äº§ç‰©",
    "relationships_formatted": "1. æ¤ç‰© -- å…‰åˆä½œç”¨: æ¤ç‰©è¿›è¡Œå…‰åˆä½œç”¨\n2. å…‰åˆä½œç”¨ -- æ°§æ°”: å…‰åˆä½œç”¨äº§ç”Ÿæ°§æ°”",
    "multi_hop_generation_prompt": "è¯·åŸºäºä»¥ä¸‹çŸ¥è¯†å­å›¾ç”Ÿæˆå¤šè·³æ¨ç†é—®é¢˜å’Œç­”æ¡ˆ...",
    "raw_response": "Question: æ¤ç‰©å¦‚ä½•é€šè¿‡å…‰åˆä½œç”¨äº§ç”Ÿæ°§æ°”ï¼Ÿ\nAnswer: æ¤ç‰©é€šè¿‡å…‰åˆä½œç”¨åˆ†è§£æ°´åˆ†å­ï¼Œé‡Šæ”¾æ°§æ°”..."
  }
}
```

## ğŸ” æ•°æ®åˆ†æç¤ºä¾‹

### Pythonåˆ†æè„šæœ¬

```python
import json
from collections import Counter

# åŠ è½½æ•°æ®
with open('cache/data/graphgen/1234567890/qa.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

# ç»Ÿè®¡å„æ¨¡å¼æ•°é‡
modes = [item['intermediate_steps']['mode'] for item in data if 'intermediate_steps' in item]
mode_counts = Counter(modes)
print("æ¨¡å¼åˆ†å¸ƒ:", mode_counts)

# åˆ†æprompté•¿åº¦
prompt_lengths = []
for item in data:
    if 'intermediate_steps' not in item:
        continue
    steps = item['intermediate_steps']
    
    # æ”¶é›†æ‰€æœ‰promptå­—æ®µ
    for key, value in steps.items():
        if 'prompt' in key and isinstance(value, str):
            prompt_lengths.append(len(value))

print(f"Promptå¹³å‡é•¿åº¦: {sum(prompt_lengths)/len(prompt_lengths):.0f} å­—ç¬¦")

# æå–CoTæ¨¡å¼çš„æ¨ç†è·¯å¾„
cot_reasoning_paths = []
for item in data:
    if item.get('intermediate_steps', {}).get('mode') == 'cot':
        if 'reasoning_path' in item:
            cot_reasoning_paths.append(item['reasoning_path'])

print(f"CoTæ¨ç†è·¯å¾„æ•°é‡: {len(cot_reasoning_paths)}")

# åˆ†æå®ä½“æ•°é‡åˆ†å¸ƒ
entity_counts = []
for item in data:
    steps = item.get('intermediate_steps', {})
    if 'entities' in steps:
        entity_counts.append(len(steps['entities']))

if entity_counts:
    print(f"å¹³å‡å®ä½“æ•°é‡: {sum(entity_counts)/len(entity_counts):.1f}")
```

### æå–ç‰¹å®šprompt

```python
import json

with open('cache/data/graphgen/1234567890/qa.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

# æå–æ‰€æœ‰atomicæ¨¡å¼çš„prompt
atomic_prompts = []
for item in data:
    steps = item.get('intermediate_steps', {})
    if steps.get('mode') == 'atomic' and 'qa_generation_prompt' in steps:
        atomic_prompts.append(steps['qa_generation_prompt'])

# ä¿å­˜åˆ°æ–‡ä»¶
with open('atomic_prompts.txt', 'w', encoding='utf-8') as f:
    for i, prompt in enumerate(atomic_prompts, 1):
        f.write(f"=== Prompt {i} ===\n")
        f.write(prompt)
        f.write("\n\n")

print(f"å·²ä¿å­˜ {len(atomic_prompts)} ä¸ªatomicæ¨¡å¼çš„prompt")
```

## ğŸ“‹ ä¿®æ”¹çš„æ–‡ä»¶æ¸…å•

### æ ¸å¿ƒæ–‡ä»¶

1. **`/workspace/graphgen/operators/traverse_graph.py`**
   - ä¿®æ”¹äº† `traverse_graph_for_atomic()` å‡½æ•°
   - ä¿®æ”¹äº† `traverse_graph_for_aggregated()` å‡½æ•°
   - ä¿®æ”¹äº† `traverse_graph_for_multi_hop()` å‡½æ•°

2. **`/workspace/graphgen/operators/generate/generate_cot.py`**
   - ä¿®æ”¹äº† `generate_cot()` å‡½æ•°
   - ä¿®æ”¹äº†å†…éƒ¨å‡½æ•° `_generate_from_single_community()`

3. **`/workspace/graphgen/utils/format.py`**
   - ä¿®æ”¹äº† `format_generation_results()` å‡½æ•°
   - ç¡®ä¿åœ¨æ‰€æœ‰è¾“å‡ºæ ¼å¼ä¸­ä¿ç•™ä¸­é—´æ­¥éª¤ä¿¡æ¯

### æ–‡æ¡£æ–‡ä»¶

- `INTERMEDIATE_STEPS_GUIDE.md`: è¯¦ç»†ä½¿ç”¨æŒ‡å—
- `INTERMEDIATE_STEPS_README.md`: åŠŸèƒ½æ€»è§ˆï¼ˆæœ¬æ–‡ä»¶ï¼‰
- `test_intermediate_steps.py`: æµ‹è¯•éªŒè¯è„šæœ¬

## âš ï¸ æ³¨æ„äº‹é¡¹

### å­˜å‚¨ç©ºé—´

ç”±äºä¿å­˜äº†å®Œæ•´çš„ä¸­é—´æ­¥éª¤ï¼Œç”Ÿæˆçš„æ–‡ä»¶ä¼š**æ˜¾è‘—å¢å¤§**ï¼š
- åŸå§‹å¤§å°: ~100KB (1000æ¡æ•°æ®)
- å¸¦ä¸­é—´æ­¥éª¤: ~500KB-1MB (1000æ¡æ•°æ®)

å»ºè®®ï¼š
- å®šæœŸæ¸…ç†ä¸éœ€è¦çš„å†å²æ•°æ®
- ç”Ÿäº§ç¯å¢ƒå¯è€ƒè™‘åªåœ¨è°ƒè¯•æ—¶å¯ç”¨
- ä½¿ç”¨å‹ç¼©å­˜å‚¨é•¿æœŸä¿å­˜çš„æ•°æ®

### å‘åå…¼å®¹æ€§

âœ… **å®Œå…¨å‘åå…¼å®¹**
- ä¸å½±å“ç°æœ‰ä»£ç çš„åŠŸèƒ½
- `intermediate_steps` æ˜¯æ–°å¢å­—æ®µï¼Œå¯é€‰ä½¿ç”¨
- å¦‚ä¸éœ€è¦ï¼Œå¯ç›´æ¥å¿½ç•¥è¯¥å­—æ®µ

### æ€§èƒ½å½±å“

- **å†…å­˜**: å¢åŠ çº¦20-30%ï¼ˆå­˜å‚¨é¢å¤–çš„promptå’Œå“åº”ï¼‰
- **ç£ç›˜IO**: å†™å…¥æ—¶é—´å¢åŠ çº¦10-20%
- **ç”Ÿæˆé€Ÿåº¦**: æ— å½±å“ï¼ˆåªæ˜¯ä¿å­˜æ›´å¤šä¿¡æ¯ï¼‰

## ğŸ› ï¸ é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰promptåˆ†æ

```python
import json
import re

def analyze_prompt_patterns(qa_json_path):
    """åˆ†æpromptä¸­ä½¿ç”¨çš„æ¨¡å¼å’Œå…³é”®è¯"""
    
    with open(qa_json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # æå–æ‰€æœ‰prompt
    all_prompts = []
    for item in data:
        steps = item.get('intermediate_steps', {})
        for key, value in steps.items():
            if 'prompt' in key and isinstance(value, str):
                all_prompts.append(value)
    
    # ç»Ÿè®¡å…³é”®è¯é¢‘ç‡
    keywords = ['question', 'answer', 'entity', 'relationship', 
                'reasoning', 'generate', 'based on']
    
    keyword_counts = {kw: 0 for kw in keywords}
    for prompt in all_prompts:
        for kw in keywords:
            keyword_counts[kw] += len(re.findall(kw, prompt, re.IGNORECASE))
    
    print("å…³é”®è¯é¢‘ç‡:")
    for kw, count in sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True):
        print(f"  {kw}: {count}")
    
    return keyword_counts

# ä½¿ç”¨
analyze_prompt_patterns('cache/data/graphgen/1234567890/qa.json')
```

### è´¨é‡è¯„ä¼°

```python
import json

def evaluate_quality(qa_json_path):
    """è¯„ä¼°ç”Ÿæˆè´¨é‡ï¼ˆåŸºäºä¸­é—´æ­¥éª¤ï¼‰"""
    
    with open(qa_json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    metrics = {
        'avg_question_length': [],
        'avg_answer_length': [],
        'avg_entities_count': [],
        'avg_relationships_count': [],
    }
    
    for item in data:
        # é—®é¢˜å’Œç­”æ¡ˆé•¿åº¦
        if 'instruction' in item:
            metrics['avg_question_length'].append(len(item['instruction']))
            metrics['avg_answer_length'].append(len(item['output']))
        
        # å®ä½“å’Œå…³ç³»æ•°é‡
        steps = item.get('intermediate_steps', {})
        if 'entities' in steps:
            metrics['avg_entities_count'].append(len(steps['entities']))
        if 'relationships' in steps:
            metrics['avg_relationships_count'].append(len(steps['relationships']))
    
    # è®¡ç®—å¹³å‡å€¼
    results = {}
    for key, values in metrics.items():
        if values:
            results[key] = sum(values) / len(values)
    
    print("è´¨é‡æŒ‡æ ‡:")
    for key, value in results.items():
        print(f"  {key}: {value:.2f}")
    
    return results

# ä½¿ç”¨
evaluate_quality('cache/data/graphgen/1234567890/qa.json')
```

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·ï¼š
1. æŸ¥çœ‹ `INTERMEDIATE_STEPS_GUIDE.md` è¯¦ç»†æ–‡æ¡£
2. è¿è¡Œ `test_intermediate_steps.py` éªŒè¯åŠŸèƒ½
3. æ£€æŸ¥ç”Ÿæˆçš„æ—¥å¿—æ–‡ä»¶

## ğŸ“ æœ€ä½³å®è·µ

1. **å¼€å‘é˜¶æ®µ**: å¯ç”¨ä¸­é—´æ­¥éª¤ä¿å­˜ï¼Œä¾¿äºè°ƒè¯•å’Œä¼˜åŒ–prompt
2. **æµ‹è¯•é˜¶æ®µ**: ä½¿ç”¨ä¸­é—´æ­¥éª¤æ•°æ®åˆ†æç”Ÿæˆè´¨é‡
3. **ç”Ÿäº§é˜¶æ®µ**: æ ¹æ®éœ€è¦å†³å®šæ˜¯å¦ä¿ç•™ï¼ˆå»ºè®®ä¿ç•™ç”¨äºæŒç»­ä¼˜åŒ–ï¼‰
4. **æ•°æ®åˆ†æ**: å®šæœŸåˆ†æä¸­é—´æ­¥éª¤ï¼Œä¼˜åŒ–promptè®¾è®¡

## ğŸ“ˆ ç‰ˆæœ¬å†å²

- **v1.0** (2025-10-27): åˆå§‹ç‰ˆæœ¬
  - æ”¯æŒatomicã€aggregatedã€multi_hopã€cotå››ç§æ¨¡å¼
  - æ”¯æŒAlpacaã€Sharegptã€ChatMLä¸‰ç§è¾“å‡ºæ ¼å¼
  - å®Œæ•´çš„promptå’Œå“åº”è¿½è¸ª
