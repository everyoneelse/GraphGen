#!/usr/bin/env python3
"""
ä¸“é—¨é’ˆå¯¹ youtu-graphrag JSON æ ¼å¼çš„çŸ¥è¯†å›¾è°±è½¬æ¢å™¨
"""

import networkx as nx
import json
import argparse
import os
from typing import Dict, Any, List, Union
from pathlib import Path


class YoutuJSONConverter:
    """å°† youtu-graphrag JSON æ ¼å¼çš„çŸ¥è¯†å›¾è°±è½¬æ¢ä¸º GraphGen æ ¼å¼"""
    
    def __init__(self):
        self.graph = nx.Graph()
        self.entity_nodes = {}  # å­˜å‚¨å®ä½“èŠ‚ç‚¹ä¿¡æ¯
        self.attribute_nodes = {}  # å­˜å‚¨å±æ€§èŠ‚ç‚¹ä¿¡æ¯
        self.community_nodes = {}  # å­˜å‚¨ç¤¾åŒºèŠ‚ç‚¹ä¿¡æ¯
        self.relations = []  # å­˜å‚¨å…³ç³»ä¿¡æ¯
        self.chunks = {}  # å­˜å‚¨ chunk ä¿¡æ¯ {chunk_id: chunk_data}
    
    def load_youtu_json_data(self, json_file: str):
        """
        åŠ è½½ youtu-graphrag JSON æ ¼å¼çš„çŸ¥è¯†å›¾è°±æ•°æ®
        
        Args:
            json_file: JSON æ–‡ä»¶è·¯å¾„
        """
        print(f"æ­£åœ¨åŠ è½½ youtu-graphrag JSON æ•°æ®: {json_file}")
        
        if not os.path.exists(json_file):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {json_file}")
        
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if not isinstance(data, list):
            raise ValueError("JSON æ•°æ®åº”è¯¥æ˜¯ä¸€ä¸ªåˆ—è¡¨æ ¼å¼")
        
        print(f"åŠ è½½å®Œæˆ - å…± {len(data)} æ¡å…³ç³»è®°å½•")
        return data
    
    def load_youtu_chunks(self, chunks_file: str):
        """
        åŠ è½½ youtu-graphrag çš„ chunks æ–‡ä»¶
        
        æ ¼å¼: id: CHUNK_ID\tChunk: {'title': '...', 'content': '...', 'source': '...'}
        
        Args:
            chunks_file: chunks æ–‡ä»¶è·¯å¾„ (é€šå¸¸æ˜¯ text æ–‡ä»¶)
        """
        print(f"æ­£åœ¨åŠ è½½ youtu-graphrag chunks æ•°æ®: {chunks_file}")
        
        if not os.path.exists(chunks_file):
            raise FileNotFoundError(f"Chunks æ–‡ä»¶ä¸å­˜åœ¨: {chunks_file}")
        
        chunks_loaded = 0
        with open(chunks_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                
                try:
                    # è§£ææ ¼å¼: id: CHUNK_ID\tChunk: {...}
                    if '\t' in line:
                        parts = line.split('\t', 1)
                        if len(parts) == 2:
                            # æå– chunk_id
                            id_part = parts[0].strip()
                            if id_part.startswith('id:'):
                                chunk_id = id_part[3:].strip()
                            else:
                                chunk_id = id_part
                            
                            # æå– chunk æ•°æ®
                            chunk_part = parts[1].strip()
                            if chunk_part.startswith('Chunk:'):
                                chunk_str = chunk_part[6:].strip()
                                # ä½¿ç”¨ eval æˆ– json.loads è§£æå­—å…¸
                                try:
                                    chunk_data = eval(chunk_str)  # å› ä¸ºæ˜¯ Python å­—å…¸æ ¼å¼
                                except:
                                    chunk_data = json.loads(chunk_str)
                                
                                self.chunks[chunk_id] = chunk_data
                                chunks_loaded += 1
                except Exception as e:
                    print(f"âš ï¸  è­¦å‘Š: æ— æ³•è§£æç¬¬ {line_num} è¡Œ: {str(e)[:50]}")
                    continue
        
        print(f"åŠ è½½å®Œæˆ - å…± {chunks_loaded} ä¸ª chunks")
        return chunks_loaded
    
    def parse_youtu_data(self, data: List[Dict]):
        """è§£æ youtu-graphrag æ•°æ®ç»“æ„"""
        print("å¼€å§‹è§£ææ•°æ®ç»“æ„...")
        
        for item in data:
            # è§£æèµ·å§‹èŠ‚ç‚¹
            start_node = item.get('start_node', {})
            start_label = start_node.get('label', '')
            start_props = start_node.get('properties', {})
            start_name = start_props.get('name', '')
            
            # è§£æç»“æŸèŠ‚ç‚¹
            end_node = item.get('end_node', {})
            end_label = end_node.get('label', '')
            end_props = end_node.get('properties', {})
            end_name = end_props.get('name', '')
            
            # è§£æå…³ç³»
            relation = item.get('relation', '')
            
            # å­˜å‚¨èŠ‚ç‚¹ä¿¡æ¯
            if start_label == 'entity' and start_name:
                self.entity_nodes[start_name] = {
                    'name': start_name,
                    'label': start_label,
                    'chunk_id': start_props.get('chunk id', ''),
                    'schema_type': start_props.get('schema_type', 'unknown'),
                    'properties': start_props
                }
            
            if end_label == 'attribute' and end_name:
                self.attribute_nodes[end_name] = {
                    'name': end_name,
                    'label': end_label,
                    'chunk_id': end_props.get('chunk id', ''),
                    'properties': end_props
                }
            elif end_label == 'community' and end_name:
                # è§£æç¤¾åŒºèŠ‚ç‚¹
                self.community_nodes[end_name] = {
                    'name': end_name,
                    'label': end_label,
                    'description': end_props.get('description', ''),
                    'members': end_props.get('members', []),
                    'properties': end_props
                }
            elif end_label == 'entity' and end_name:
                self.entity_nodes[end_name] = {
                    'name': end_name,
                    'label': end_label,
                    'chunk_id': end_props.get('chunk id', ''),
                    'schema_type': end_props.get('schema_type', 'unknown'),
                    'properties': end_props
                }
            
            # å­˜å‚¨å…³ç³»ä¿¡æ¯
            if start_name and end_name and relation:
                self.relations.append({
                    'source': start_name,
                    'target': end_name,
                    'relation': relation,
                    'source_type': start_label,
                    'target_type': end_label
                })
        
        print(f"è§£æå®Œæˆ:")
        print(f"  - å®ä½“èŠ‚ç‚¹: {len(self.entity_nodes)}")
        print(f"  - å±æ€§èŠ‚ç‚¹: {len(self.attribute_nodes)}")
        print(f"  - ç¤¾åŒºèŠ‚ç‚¹: {len(self.community_nodes)}")
        print(f"  - å…³ç³»: {len(self.relations)}")
    
    def convert_to_graphgen_format(self):
        """è½¬æ¢ä¸º GraphGen å…¼å®¹çš„æ ¼å¼"""
        print("å¼€å§‹è½¬æ¢ä¸º GraphGen æ ¼å¼...")
        
        # æ·»åŠ å®ä½“èŠ‚ç‚¹
        for entity_name, entity_data in self.entity_nodes.items():
            # æ”¶é›†è¯¥å®ä½“çš„æ‰€æœ‰å±æ€§
            entity_attributes = []
            for relation in self.relations:
                if (relation['source'] == entity_name and 
                    relation['relation'] == 'has_attribute' and
                    relation['target_type'] == 'attribute'):
                    attr_name = relation['target']
                    if attr_name in self.attribute_nodes:
                        entity_attributes.append(attr_name)
            
            # æ„å»ºå®ä½“æè¿°
            description_parts = []
            if entity_data['schema_type'] != 'unknown':
                description_parts.append(f"Type: {entity_data['schema_type']}")
            
            # æ·»åŠ å±æ€§ä¿¡æ¯åˆ°æè¿°ä¸­
            for attr_name in entity_attributes:
                if attr_name in self.attribute_nodes:
                    attr_info = self.attribute_nodes[attr_name]['name']
                    description_parts.append(attr_info)
            
            description = "; ".join(description_parts) if description_parts else f"Entity: {entity_name}"
            
            # æ·»åŠ èŠ‚ç‚¹åˆ°å›¾ä¸­
            node_data = {
                'entity_name': entity_name,
                'entity_type': entity_data['schema_type'],
                'description': description,
                'source_id': entity_data['chunk_id'] or 'youtu_graphrag',
                'original_label': entity_data['label']
            }
            
            self.graph.add_node(entity_name, **node_data)
        
        print(f"å·²æ·»åŠ  {self.graph.number_of_nodes()} ä¸ªå®ä½“èŠ‚ç‚¹")
        
        # æ·»åŠ å®ä½“é—´çš„å…³ç³»ï¼ˆè·³è¿‡ has_attribute å…³ç³»ï¼Œå› ä¸ºå·²ç»æ•´åˆåˆ°å®ä½“æè¿°ä¸­ï¼‰
        entity_relations = []
        for relation in self.relations:
            if (relation['source_type'] == 'entity' and 
                relation['target_type'] == 'entity' and
                relation['relation'] != 'has_attribute'):
                entity_relations.append(relation)
        
        # æ·»åŠ è¾¹
        valid_edges = 0
        for relation in entity_relations:
            source = relation['source']
            target = relation['target']
            relation_type = relation['relation']
            
            if source in self.entity_nodes and target in self.entity_nodes:
                edge_data = {
                    'weight': 1.0,
                    'description': f"{source} {relation_type} {target}",
                    'relation_type': relation_type,
                    'source_id': 'youtu_graphrag'
                }
                
                self.graph.add_edge(source, target, **edge_data)
                valid_edges += 1
        
        print(f"å·²æ·»åŠ  {valid_edges} æ¡å®ä½“é—´å…³ç³»")
        
        # å¦‚æœæ²¡æœ‰å®ä½“é—´å…³ç³»ï¼Œåˆ›å»ºä¸€äº›åŸºäºå…±åŒå±æ€§çš„å…³ç³»
        if valid_edges == 0:
            print("æ²¡æœ‰å‘ç°å®ä½“é—´å…³ç³»ï¼ŒåŸºäºå…±åŒå±æ€§åˆ›å»ºå…³ç³»...")
            self._create_attribute_based_relations()
    
    def _create_attribute_based_relations(self):
        """åŸºäºå…±åŒå±æ€§åˆ›å»ºå®ä½“é—´å…³ç³»"""
        # æŒ‰ chunk_id åˆ†ç»„å®ä½“
        chunk_groups = {}
        for entity_name, entity_data in self.entity_nodes.items():
            chunk_id = entity_data['chunk_id']
            if chunk_id:
                if chunk_id not in chunk_groups:
                    chunk_groups[chunk_id] = []
                chunk_groups[chunk_id].append(entity_name)
        
        # ä¸ºåŒä¸€ chunk ä¸­çš„å®ä½“åˆ›å»ºå…³ç³»
        edges_added = 0
        for chunk_id, entities in chunk_groups.items():
            if len(entities) > 1:
                # ä¸ºæ¯å¯¹å®ä½“åˆ›å»ºå…³ç³»
                for i in range(len(entities)):
                    for j in range(i + 1, len(entities)):
                        entity1, entity2 = entities[i], entities[j]
                        
                        edge_data = {
                            'weight': 0.8,
                            'description': f"{entity1} and {entity2} appear in the same context",
                            'relation_type': 'co_occurrence',
                            'source_id': chunk_id
                        }
                        
                        self.graph.add_edge(entity1, entity2, **edge_data)
                        edges_added += 1
        
        print(f"åŸºäºå…±åŒä¸Šä¸‹æ–‡æ·»åŠ äº† {edges_added} æ¡å…³ç³»")
    
    def save_to_graphml(self, output_file: str):
        """ä¿å­˜ä¸º GraphML æ ¼å¼"""
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        # ç¡®ä¿æ‰€æœ‰å±æ€§éƒ½æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼ˆGraphML è¦æ±‚ï¼‰
        for node_id, node_data in self.graph.nodes(data=True):
            for key, value in node_data.items():
                node_data[key] = str(value)
        
        for source, target, edge_data in self.graph.edges(data=True):
            for key, value in edge_data.items():
                edge_data[key] = str(value)
        
        nx.write_graphml(self.graph, output_file, encoding='utf-8')
        print(f"\nâœ… çŸ¥è¯†å›¾è°±è½¬æ¢å®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   - èŠ‚ç‚¹æ•°: {self.graph.number_of_nodes()}")
        print(f"   - è¾¹æ•°: {self.graph.number_of_edges()}")
        
        # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹èŠ‚ç‚¹å’Œè¾¹
        self._show_examples()
    
    def _show_examples(self):
        """æ˜¾ç¤ºç¤ºä¾‹èŠ‚ç‚¹å’Œè¾¹"""
        if self.graph.number_of_nodes() > 0:
            print(f"\nğŸ“ ç¤ºä¾‹èŠ‚ç‚¹:")
            for i, (node_id, node_data) in enumerate(self.graph.nodes(data=True)):
                if i >= 3:  # åªæ˜¾ç¤ºå‰3ä¸ª
                    break
                entity_type = node_data.get('entity_type', 'unknown')
                description = node_data.get('description', '')[:80]
                print(f"   - {node_id} ({entity_type}): {description}...")
        
        if self.graph.number_of_edges() > 0:
            print(f"\nğŸ”— ç¤ºä¾‹å…³ç³»:")
            for i, (source, target, edge_data) in enumerate(self.graph.edges(data=True)):
                if i >= 3:  # åªæ˜¾ç¤ºå‰3ä¸ª
                    break
                relation_type = edge_data.get('relation_type', 'unknown')
                print(f"   - {source} --[{relation_type}]--> {target}")
    
    def validate_graph(self):
        """éªŒè¯è½¬æ¢åçš„å›¾è°±"""
        issues = []
        
        # æ£€æŸ¥å­¤ç«‹èŠ‚ç‚¹
        isolated_nodes = list(nx.isolates(self.graph))
        if isolated_nodes:
            issues.append(f"å‘ç° {len(isolated_nodes)} ä¸ªå­¤ç«‹èŠ‚ç‚¹")
        
        # æ£€æŸ¥èŠ‚ç‚¹å±æ€§
        nodes_without_description = 0
        for node_id, node_data in self.graph.nodes(data=True):
            if not node_data.get('description', '').strip():
                nodes_without_description += 1
        
        if nodes_without_description > 0:
            issues.append(f"{nodes_without_description} ä¸ªèŠ‚ç‚¹ç¼ºå°‘æè¿°")
        
        # æ£€æŸ¥è¿é€šæ€§
        if self.graph.number_of_nodes() > 1:
            connected_components = list(nx.connected_components(self.graph))
            if len(connected_components) > 1:
                largest_component = max(len(cc) for cc in connected_components)
                issues.append(f"å›¾è°±æœ‰ {len(connected_components)} ä¸ªè¿é€šç»„ä»¶ï¼Œæœ€å¤§ç»„ä»¶åŒ…å« {largest_component} ä¸ªèŠ‚ç‚¹")
        
        if issues:
            print(f"\nâš ï¸  éªŒè¯å‘ç°çš„é—®é¢˜:")
            for issue in issues:
                print(f"   - {issue}")
        else:
            print(f"\nâœ… å›¾è°±éªŒè¯é€šè¿‡ï¼Œæ²¡æœ‰å‘ç°é—®é¢˜")
    
    def export_statistics(self, output_file: str):
        """å¯¼å‡ºè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'basic_info': {
                'nodes': self.graph.number_of_nodes(),
                'edges': self.graph.number_of_edges(),
                'density': nx.density(self.graph) if self.graph.number_of_nodes() > 1 else 0
            },
            'entity_types': {},
            'relation_types': {},
            'chunk_distribution': {},
            'communities': len(self.community_nodes)
        }
        
        # ç»Ÿè®¡å®ä½“ç±»å‹
        for node_id, node_data in self.graph.nodes(data=True):
            entity_type = node_data.get('entity_type', 'unknown')
            stats['entity_types'][entity_type] = stats['entity_types'].get(entity_type, 0) + 1
        
        # ç»Ÿè®¡å…³ç³»ç±»å‹
        for source, target, edge_data in self.graph.edges(data=True):
            relation_type = edge_data.get('relation_type', 'unknown')
            stats['relation_types'][relation_type] = stats['relation_types'].get(relation_type, 0) + 1
        
        # ç»Ÿè®¡ chunk åˆ†å¸ƒ
        for node_id, node_data in self.graph.nodes(data=True):
            chunk_id = node_data.get('source_id', 'unknown')
            stats['chunk_distribution'][chunk_id] = stats['chunk_distribution'].get(chunk_id, 0) + 1
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯å·²å¯¼å‡ºåˆ°: {output_file}")
    
    def export_communities(self, output_file: str):
        """å¯¼å‡ºç¤¾åŒºä¿¡æ¯ä¸ºJSONæ ¼å¼"""
        communities_data = []
        for comm_name, comm_data in self.community_nodes.items():
            communities_data.append({
                'name': comm_name,
                'description': comm_data['description'],
                'members': comm_data['members'],
                'member_count': len(comm_data['members'])
            })
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(communities_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ˜ï¸  ç¤¾åŒºä¿¡æ¯å·²å¯¼å‡ºåˆ°: {output_file}")
        print(f"   - ç¤¾åŒºæ•°é‡: {len(communities_data)}")
        
        return communities_data
    
    def get_communities_dict(self) -> Dict[str, int]:
        """
        å°†ç¤¾åŒºä¿¡æ¯è½¬æ¢ä¸º CommunityDetector æ ¼å¼
        è¿”å›: {node_name: community_id} çš„å­—å…¸
        """
        communities_dict = {}
        
        for comm_id, (comm_name, comm_data) in enumerate(self.community_nodes.items()):
            members = comm_data.get('members', [])
            for member in members:
                communities_dict[member] = comm_id
        
        return communities_dict
    
    def get_chunks_dict(self) -> Dict[str, Dict]:
        """
        è·å– chunks å­—å…¸ï¼Œç”¨äºåœ¨ç”Ÿæˆæ—¶æä¾›æ–‡æ¡£ä¸Šä¸‹æ–‡
        
        Returns:
            Dict[str, Dict]: {chunk_id: {'content': ..., 'title': ..., 'source': ...}}
        """
        return self.chunks.copy()
    
    def export_chunks(self, output_file: str):
        """å¯¼å‡º chunks ä¿¡æ¯ä¸º JSON æ ¼å¼"""
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.chunks, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ Chunks ä¿¡æ¯å·²å¯¼å‡ºåˆ°: {output_file}")
        print(f"   - Chunks æ•°é‡: {len(self.chunks)}")
        
        return self.chunks


def main():
    parser = argparse.ArgumentParser(description='å°† youtu-graphrag JSON çŸ¥è¯†å›¾è°±è½¬æ¢ä¸º GraphGen æ ¼å¼')
    parser.add_argument('--input', required=True, help='è¾“å…¥ JSON æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', required=True, help='è¾“å‡º GraphML æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--validate', action='store_true', help='éªŒè¯è½¬æ¢åçš„å›¾è°±')
    parser.add_argument('--stats', help='å¯¼å‡ºç»Ÿè®¡ä¿¡æ¯åˆ°æŒ‡å®šæ–‡ä»¶')
    
    args = parser.parse_args()
    
    try:
        converter = YoutuJSONConverter()
        data = converter.load_youtu_json_data(args.input)
        converter.parse_youtu_data(data)
        converter.convert_to_graphgen_format()
        
        if args.validate:
            converter.validate_graph()
        
        converter.save_to_graphml(args.output)
        
        if args.stats:
            converter.export_statistics(args.stats)
        
    except Exception as e:
        print(f"âŒ è½¬æ¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())