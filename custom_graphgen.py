#!/usr/bin/env python3
"""
è‡ªå®šä¹‰ GraphGen ç±»ï¼Œæ”¯æŒä½¿ç”¨å¤–éƒ¨çŸ¥è¯†å›¾è°±
"""

import os
import time
import networkx as nx
from dataclasses import dataclass
from typing import Dict

from graphgen.graphgen import GraphGen
from graphgen.models.storage.networkx_storage import NetworkXStorage
from graphgen.utils import logger


@dataclass
class CustomGraphGen(GraphGen):
    """æ”¯æŒå¤–éƒ¨çŸ¥è¯†å›¾è°±çš„ GraphGen"""
    
    external_graph_path: str = None
    skip_kg_building: bool = True  # è·³è¿‡çŸ¥è¯†å›¾è°±æ„å»ºæ­¥éª¤
    
    def __post_init__(self):
        """åˆå§‹åŒ–ï¼ŒåŠ è½½å¤–éƒ¨çŸ¥è¯†å›¾è°±"""
        # å…ˆè°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__post_init__()
        
        # å¦‚æœæä¾›äº†å¤–éƒ¨å›¾è°±è·¯å¾„ï¼Œåˆ™åŠ è½½å®ƒ
        if self.external_graph_path:
            self._load_external_graph()
    
    def _load_external_graph(self):
        """åŠ è½½å¤–éƒ¨çŸ¥è¯†å›¾è°±"""
        if not os.path.exists(self.external_graph_path):
            raise FileNotFoundError(f"å¤–éƒ¨çŸ¥è¯†å›¾è°±æ–‡ä»¶ä¸å­˜åœ¨: {self.external_graph_path}")
        
        try:
            logger.info(f"æ­£åœ¨åŠ è½½å¤–éƒ¨çŸ¥è¯†å›¾è°±: {self.external_graph_path}")
            external_graph = nx.read_graphml(self.external_graph_path)
            
            # æ›¿æ¢é»˜è®¤çš„å›¾å­˜å‚¨
            self.graph_storage._graph = external_graph
            
            logger.info(
                f"âœ… å¤–éƒ¨çŸ¥è¯†å›¾è°±åŠ è½½æˆåŠŸ: {external_graph.number_of_nodes()} ä¸ªèŠ‚ç‚¹, "
                f"{external_graph.number_of_edges()} æ¡è¾¹"
            )
            
            # æ˜¾ç¤ºä¸€äº›ç»Ÿè®¡ä¿¡æ¯
            self._show_graph_stats(external_graph)
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½å¤–éƒ¨çŸ¥è¯†å›¾è°±å¤±è´¥: {e}")
            raise
    
    def _show_graph_stats(self, graph: nx.Graph):
        """æ˜¾ç¤ºå›¾è°±ç»Ÿè®¡ä¿¡æ¯"""
        # èŠ‚ç‚¹ç±»å‹ç»Ÿè®¡
        node_types = {}
        for node_id, node_data in graph.nodes(data=True):
            entity_type = node_data.get('entity_type', 'unknown')
            node_types[entity_type] = node_types.get(entity_type, 0) + 1
        
        logger.info("ğŸ“Š èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ:")
        for entity_type, count in sorted(node_types.items(), key=lambda x: x[1], reverse=True)[:10]:
            logger.info(f"   - {entity_type}: {count}")
        
        # è¿é€šæ€§ç»Ÿè®¡
        if graph.number_of_nodes() > 0:
            connected_components = list(nx.connected_components(graph))
            logger.info(f"ğŸ”— è¿é€šç»„ä»¶æ•°: {len(connected_components)}")
            if connected_components:
                largest_component_size = max(len(cc) for cc in connected_components)
                logger.info(f"ğŸ“ˆ æœ€å¤§è¿é€šç»„ä»¶å¤§å°: {largest_component_size}")
    
    async def insert(self, read_config: Dict = None, split_config: Dict = None):
        """
        é‡å†™æ’å…¥æ–¹æ³•ï¼Œæ”¯æŒè·³è¿‡çŸ¥è¯†å›¾è°±æ„å»º
        """
        if self.skip_kg_building and self.external_graph_path:
            logger.info("â­ï¸  è·³è¿‡çŸ¥è¯†å›¾è°±æ„å»ºæ­¥éª¤ï¼Œä½¿ç”¨å¤–éƒ¨çŸ¥è¯†å›¾è°±")
            # ç›´æ¥è°ƒç”¨æ’å…¥å®Œæˆå›è°ƒ
            await self._insert_done()
            return True
        else:
            # è°ƒç”¨åŸå§‹çš„æ’å…¥æ–¹æ³•
            return await super().insert(read_config, split_config)
    
    async def insert_additional_data(self, read_config: Dict, split_config: Dict):
        """
        åœ¨å¤–éƒ¨çŸ¥è¯†å›¾è°±åŸºç¡€ä¸Šæ’å…¥é¢å¤–æ•°æ®
        """
        logger.info("ğŸ“ åœ¨å¤–éƒ¨çŸ¥è¯†å›¾è°±åŸºç¡€ä¸Šæ’å…¥é¢å¤–æ•°æ®...")
        return await super().insert(read_config, split_config)
    
    def get_graph_summary(self) -> Dict:
        """è·å–å›¾è°±æ‘˜è¦ä¿¡æ¯"""
        graph = self.graph_storage._graph
        
        # åŸºæœ¬ç»Ÿè®¡
        summary = {
            'nodes': graph.number_of_nodes(),
            'edges': graph.number_of_edges(),
            'density': nx.density(graph) if graph.number_of_nodes() > 1 else 0,
        }
        
        # èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ
        node_types = {}
        for node_id, node_data in graph.nodes(data=True):
            entity_type = node_data.get('entity_type', 'unknown')
            node_types[entity_type] = node_types.get(entity_type, 0) + 1
        summary['node_types'] = node_types
        
        # è¿é€šæ€§ä¿¡æ¯
        if graph.number_of_nodes() > 0:
            connected_components = list(nx.connected_components(graph))
            summary['connected_components'] = len(connected_components)
            if connected_components:
                summary['largest_component_size'] = max(len(cc) for cc in connected_components)
        
        return summary
    
    def export_graph_statistics(self, output_file: str):
        """å¯¼å‡ºå›¾è°±ç»Ÿè®¡ä¿¡æ¯åˆ°æ–‡ä»¶"""
        import json
        
        summary = self.get_graph_summary()
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“Š å›¾è°±ç»Ÿè®¡ä¿¡æ¯å·²å¯¼å‡ºåˆ°: {output_file}")


def create_custom_config(
    external_graph_path: str,
    generation_mode: str = "atomic",
    data_format: str = "Alpaca",
    quiz_samples: int = 5,
    max_depth: int = 3,
    max_extra_edges: int = 5
) -> Dict:
    """
    åˆ›å»ºé€‚ç”¨äºå¤–éƒ¨çŸ¥è¯†å›¾è°±çš„é…ç½®
    
    Args:
        external_graph_path: å¤–éƒ¨çŸ¥è¯†å›¾è°±è·¯å¾„
        generation_mode: ç”Ÿæˆæ¨¡å¼ (atomic, aggregated, multi_hop, cot)
        data_format: æ•°æ®æ ¼å¼ (Alpaca, Sharegpt, ChatML)
        quiz_samples: æµ‹è¯•æ ·æœ¬æ•°é‡
        max_depth: æœ€å¤§éå†æ·±åº¦
        max_extra_edges: æœ€å¤§é¢å¤–è¾¹æ•°
    """
    config = {
        "read": {
            "input_file": []  # ç©ºæ–‡ä»¶åˆ—è¡¨ï¼Œå› ä¸ºä½¿ç”¨å¤–éƒ¨çŸ¥è¯†å›¾è°±
        },
        "split": {
            "chunk_size": 1024,
            "chunk_overlap": 100
        },
        "search": {
            "enabled": False,  # ç¦ç”¨æœç´¢ï¼Œç›´æ¥ä½¿ç”¨å¤–éƒ¨çŸ¥è¯†å›¾è°±
            "search_types": []
        },
        "quiz_and_judge": {
            "enabled": True,
            "quiz_samples": quiz_samples,
            "re_judge": False
        },
        "partition": {
            "method": "ece",
            "method_params": {
                "bidirectional": True,
                "edge_sampling": "max_loss",
                "expand_method": "max_width",
                "isolated_node_strategy": "ignore",
                "max_depth": max_depth,
                "max_extra_edges": max_extra_edges,
                "max_tokens": 256,
                "loss_strategy": "only_edge"
            }
        },
        "generate": {
            "mode": generation_mode,
            "data_format": data_format
        }
    }
    
    return config


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    import yaml
    import asyncio
    from dotenv import load_dotenv
    
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    
    # é…ç½®å‚æ•°
    EXTERNAL_GRAPH_PATH = "cache/external_graph.graphml"
    WORKING_DIR = "cache"
    
    async def main():
        # åˆ›å»ºé…ç½®
        config = create_custom_config(
            external_graph_path=EXTERNAL_GRAPH_PATH,
            generation_mode="atomic",
            data_format="Alpaca",
            quiz_samples=3,
            max_depth=2,
            max_extra_edges=3
        )
        
        # åˆ›å»ºè‡ªå®šä¹‰ GraphGen å®ä¾‹
        graph_gen = CustomGraphGen(
            external_graph_path=EXTERNAL_GRAPH_PATH,
            working_dir=WORKING_DIR,
            unique_id=int(time.time())
        )
        
        try:
            # æ’å…¥æ­¥éª¤ï¼ˆè·³è¿‡çŸ¥è¯†å›¾è°±æ„å»ºï¼‰
            await graph_gen.insert(
                read_config=config["read"], 
                split_config=config["split"]
            )
            
            # å¯é€‰ï¼šæœç´¢å¢å¼º
            if config["search"]["enabled"]:
                await graph_gen.search(search_config=config["search"])
            
            # é—®ç­”æµ‹è¯•å’Œåˆ¤æ–­
            if config["quiz_and_judge"]["enabled"]:
                await graph_gen.quiz_and_judge(quiz_and_judge_config=config["quiz_and_judge"])
            
            # ç”Ÿæˆæ•°æ®
            await graph_gen.generate(
                partition_config=config["partition"],
                generate_config=config["generate"]
            )
            
            # å¯¼å‡ºç»Ÿè®¡ä¿¡æ¯
            graph_gen.export_graph_statistics("cache/graph_statistics.json")
            
            print("âœ… æ•°æ®ç”Ÿæˆå®Œæˆï¼æ£€æŸ¥ cache/data/graphgen/ ç›®å½•")
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            raise
    
    # è¿è¡Œç¤ºä¾‹
    # asyncio.run(main())