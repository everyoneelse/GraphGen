#!/usr/bin/env python3
"""
æµ‹è¯•ä¸­é—´æ­¥éª¤ä¿å­˜åŠŸèƒ½çš„è„šæœ¬

ä½¿ç”¨æ–¹æ³•:
python test_intermediate_steps.py <qa_json_path>

ä¾‹å¦‚:
python test_intermediate_steps.py cache/data/graphgen/1234567890/qa.json
"""

import json
import sys
from pathlib import Path


def test_intermediate_steps(qa_json_path):
    """æµ‹è¯•qa.jsonä¸­æ˜¯å¦åŒ…å«ä¸­é—´æ­¥éª¤ä¿¡æ¯"""
    
    print(f"ğŸ” æ­£åœ¨æ£€æŸ¥: {qa_json_path}\n")
    
    if not Path(qa_json_path).exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {qa_json_path}")
        return False
    
    # åŠ è½½æ•°æ®
    with open(qa_json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    if not data:
        print("âŒ æ–‡ä»¶ä¸ºç©º")
        return False
    
    print(f"âœ… æˆåŠŸåŠ è½½ {len(data)} æ¡æ•°æ®\n")
    
    # ç»Ÿè®¡å„æ¨¡å¼çš„æ•°æ®
    mode_counts = {}
    has_intermediate_steps = 0
    
    for idx, item in enumerate(data):
        # æ£€æŸ¥intermediate_stepså­—æ®µ
        if "intermediate_steps" in item:
            has_intermediate_steps += 1
            mode = item["intermediate_steps"].get("mode", "unknown")
            mode_counts[mode] = mode_counts.get(mode, 0) + 1
    
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   - æ€»æ•°æ®é‡: {len(data)}")
    print(f"   - åŒ…å«ä¸­é—´æ­¥éª¤çš„æ•°æ®: {has_intermediate_steps} ({has_intermediate_steps/len(data)*100:.1f}%)")
    print(f"\nğŸ“ å„æ¨¡å¼åˆ†å¸ƒ:")
    for mode, count in sorted(mode_counts.items()):
        print(f"   - {mode}: {count}")
    
    # æ˜¾ç¤ºç¬¬ä¸€æ¡æ•°æ®çš„è¯¦ç»†ä¿¡æ¯
    if data:
        print(f"\nğŸ“„ ç¬¬ä¸€æ¡æ•°æ®ç¤ºä¾‹:")
        first_item = data[0]
        
        # æ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
        if "instruction" in first_item:
            print(f"\n   é—®é¢˜: {first_item['instruction'][:100]}...")
            print(f"   ç­”æ¡ˆ: {first_item.get('output', 'N/A')[:100]}...")
        elif "conversations" in first_item:
            print(f"\n   é—®é¢˜: {first_item['conversations'][0]['value'][:100]}...")
            print(f"   ç­”æ¡ˆ: {first_item['conversations'][1]['value'][:100]}...")
        elif "messages" in first_item:
            print(f"\n   é—®é¢˜: {first_item['messages'][0]['content'][:100]}...")
            print(f"   ç­”æ¡ˆ: {first_item['messages'][1]['content'][:100]}...")
        
        # æ˜¾ç¤ºä¸­é—´æ­¥éª¤ä¿¡æ¯
        if "intermediate_steps" in first_item:
            print(f"\n   âœ… åŒ…å«ä¸­é—´æ­¥éª¤ä¿¡æ¯")
            steps = first_item["intermediate_steps"]
            mode = steps.get("mode", "unknown")
            print(f"   æ¨¡å¼: {mode}")
            
            # æ ¹æ®æ¨¡å¼æ˜¾ç¤ºä¸åŒçš„å­—æ®µ
            if mode == "atomic":
                print(f"\n   Atomicæ¨¡å¼å­—æ®µ:")
                print(f"     - input_description: {'âœ…' if 'input_description' in steps else 'âŒ'}")
                print(f"     - qa_generation_prompt: {'âœ…' if 'qa_generation_prompt' in steps else 'âŒ'}")
                print(f"     - raw_qa_response: {'âœ…' if 'raw_qa_response' in steps else 'âŒ'}")
                
                if 'qa_generation_prompt' in steps:
                    print(f"\n   Prompté¢„è§ˆ:")
                    print(f"     {steps['qa_generation_prompt'][:200]}...")
            
            elif mode == "cot":
                print(f"\n   CoTæ¨¡å¼å­—æ®µ:")
                print(f"     - entities: {'âœ…' if 'entities' in steps else 'âŒ'}")
                print(f"     - relationships: {'âœ…' if 'relationships' in steps else 'âŒ'}")
                print(f"     - step1_template_design_prompt: {'âœ…' if 'step1_template_design_prompt' in steps else 'âŒ'}")
                print(f"     - step1_template_design_response: {'âœ…' if 'step1_template_design_response' in steps else 'âŒ'}")
                print(f"     - step2_answer_generation_prompt: {'âœ…' if 'step2_answer_generation_prompt' in steps else 'âŒ'}")
                print(f"     - step2_final_answer: {'âœ…' if 'step2_final_answer' in steps else 'âŒ'}")
                
                if 'reasoning_path' in first_item:
                    print(f"\n   æ¨ç†è·¯å¾„: {first_item['reasoning_path'][:200]}...")
            
            elif mode in ["aggregated", "aggregated_multi"]:
                print(f"\n   Aggregatedæ¨¡å¼å­—æ®µ:")
                print(f"     - entities: {'âœ…' if 'entities' in steps else 'âŒ'}")
                print(f"     - relationships: {'âœ…' if 'relationships' in steps else 'âŒ'}")
                print(f"     - step1_rephrasing_prompt: {'âœ…' if 'step1_rephrasing_prompt' in steps else 'âŒ'}")
                print(f"     - step1_rephrased_context: {'âœ…' if 'step1_rephrased_context' in steps else 'âŒ'}")
                
                if mode == "aggregated":
                    print(f"     - step2_question_generation_prompt: {'âœ…' if 'step2_question_generation_prompt' in steps else 'âŒ'}")
                else:
                    print(f"     - step2_multi_qa_generation_prompt: {'âœ…' if 'step2_multi_qa_generation_prompt' in steps else 'âŒ'}")
            
            elif mode == "multi_hop":
                print(f"\n   Multi-hopæ¨¡å¼å­—æ®µ:")
                print(f"     - entities: {'âœ…' if 'entities' in steps else 'âŒ'}")
                print(f"     - relationships: {'âœ…' if 'relationships' in steps else 'âŒ'}")
                print(f"     - multi_hop_generation_prompt: {'âœ…' if 'multi_hop_generation_prompt' in steps else 'âŒ'}")
                print(f"     - raw_response: {'âœ…' if 'raw_response' in steps else 'âŒ'}")
        else:
            print(f"\n   âŒ ä¸åŒ…å«ä¸­é—´æ­¥éª¤ä¿¡æ¯")
    
    print("\n" + "="*60)
    
    if has_intermediate_steps == len(data):
        print("âœ… æµ‹è¯•é€šè¿‡ï¼šæ‰€æœ‰æ•°æ®éƒ½åŒ…å«ä¸­é—´æ­¥éª¤ä¿¡æ¯")
        return True
    elif has_intermediate_steps > 0:
        print(f"âš ï¸  éƒ¨åˆ†é€šè¿‡ï¼šåªæœ‰ {has_intermediate_steps}/{len(data)} æ¡æ•°æ®åŒ…å«ä¸­é—´æ­¥éª¤")
        return False
    else:
        print("âŒ æµ‹è¯•å¤±è´¥ï¼šæ²¡æœ‰æ•°æ®åŒ…å«ä¸­é—´æ­¥éª¤ä¿¡æ¯")
        return False


def analyze_prompts(qa_json_path):
    """åˆ†ææ‰€æœ‰promptçš„ç»Ÿè®¡ä¿¡æ¯"""
    
    print(f"\nğŸ“ˆ åˆ†æPromptç»Ÿè®¡ä¿¡æ¯...\n")
    
    with open(qa_json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    prompt_lengths = []
    prompt_types = {}
    
    for item in data:
        if "intermediate_steps" not in item:
            continue
        
        steps = item["intermediate_steps"]
        mode = steps.get("mode", "unknown")
        
        # æ”¶é›†prompt
        if mode == "atomic":
            if "qa_generation_prompt" in steps:
                prompt = steps["qa_generation_prompt"]
                prompt_lengths.append(len(prompt))
                prompt_types[f"{mode}_qa"] = prompt_types.get(f"{mode}_qa", 0) + 1
        
        elif mode == "cot":
            if "step1_template_design_prompt" in steps:
                prompt = steps["step1_template_design_prompt"]
                prompt_lengths.append(len(prompt))
                prompt_types[f"{mode}_step1"] = prompt_types.get(f"{mode}_step1", 0) + 1
            
            if "step2_answer_generation_prompt" in steps:
                prompt = steps["step2_answer_generation_prompt"]
                prompt_lengths.append(len(prompt))
                prompt_types[f"{mode}_step2"] = prompt_types.get(f"{mode}_step2", 0) + 1
        
        elif mode in ["aggregated", "aggregated_multi"]:
            if "step1_rephrasing_prompt" in steps:
                prompt = steps["step1_rephrasing_prompt"]
                prompt_lengths.append(len(prompt))
                prompt_types[f"{mode}_step1"] = prompt_types.get(f"{mode}_step1", 0) + 1
            
            if "step2_question_generation_prompt" in steps:
                prompt = steps["step2_question_generation_prompt"]
                prompt_lengths.append(len(prompt))
                prompt_types[f"{mode}_step2_single"] = prompt_types.get(f"{mode}_step2_single", 0) + 1
            
            if "step2_multi_qa_generation_prompt" in steps:
                prompt = steps["step2_multi_qa_generation_prompt"]
                prompt_lengths.append(len(prompt))
                prompt_types[f"{mode}_step2_multi"] = prompt_types.get(f"{mode}_step2_multi", 0) + 1
        
        elif mode == "multi_hop":
            if "multi_hop_generation_prompt" in steps:
                prompt = steps["multi_hop_generation_prompt"]
                prompt_lengths.append(len(prompt))
                prompt_types[f"{mode}"] = prompt_types.get(f"{mode}", 0) + 1
    
    if prompt_lengths:
        print(f"Prompté•¿åº¦ç»Ÿè®¡:")
        print(f"  - æ€»æ•°é‡: {len(prompt_lengths)}")
        print(f"  - å¹³å‡é•¿åº¦: {sum(prompt_lengths)/len(prompt_lengths):.0f} å­—ç¬¦")
        print(f"  - æœ€å°é•¿åº¦: {min(prompt_lengths)} å­—ç¬¦")
        print(f"  - æœ€å¤§é•¿åº¦: {max(prompt_lengths)} å­—ç¬¦")
        
        print(f"\nPromptç±»å‹åˆ†å¸ƒ:")
        for ptype, count in sorted(prompt_types.items()):
            print(f"  - {ptype}: {count}")
    else:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•prompt")


def main():
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python test_intermediate_steps.py <qa_json_path>")
        print("\nç¤ºä¾‹:")
        print("  python test_intermediate_steps.py cache/data/graphgen/1234567890/qa.json")
        sys.exit(1)
    
    qa_json_path = sys.argv[1]
    
    # è¿è¡Œæµ‹è¯•
    success = test_intermediate_steps(qa_json_path)
    
    # å¦‚æœæµ‹è¯•é€šè¿‡ï¼Œè¿›è¡Œè¯¦ç»†åˆ†æ
    if success:
        analyze_prompts(qa_json_path)
    
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
